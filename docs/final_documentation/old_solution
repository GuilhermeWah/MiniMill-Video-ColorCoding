# MillPresenter Codebase Reference (For Cross-Codebase Comparison)

## Purpose of This Document

This document is a **single, comparison-friendly reference** for the MillPresenter codebase: what problem it solves, the constraints it optimizes for, and the exact pipeline and module responsibilities.

It is written to enable a side-by-side comparison with another codebase solving the same task (bead detection + size overlays on mill videos).

---

## Problem Statement

Given a high-FPS video of a grinding mill interior, detect hollow-ring steel beads and provide a presentation UI that can:

- Play video smoothly (target: ~60 FPS).
- Toggle overlay visibility per size class (4/6/8/10 mm) instantly.
- Export an MP4 with overlays that matches the UI.

The system is intentionally **presentation-first**: smooth playback and trustworthy overlays are prioritized over research-grade metrology.

---

## Key Architecture Decision: “Scan Once, Play Forever”

MillPresenter separates the world into two flows:

1. **Offline detection pass** (heavy compute): decode video → detect circles → classify → persist per-frame results.
2. **Live playback / export** (light compute): decode video → look up cached results → draw circles.

**Invariant:** playback toggles (4/6/8/10) must not trigger any re-detection.

---

## Repository Layout (What Lives Where)

### Source

- `src/mill_presenter/app.py`
  - CLI entry point for the UI player.
  - Loads YAML config + instantiates `FrameLoader`, `ResultsCache`, and `MainWindow`.

- `src/mill_presenter/core/`
  - `playback.py`: video decoding + rotation handling (`FrameLoader`).
  - `processor.py`: vision pipeline (`VisionProcessor`).
  - `orchestrator.py`: offline loop coordinator (`ProcessorOrchestrator`).
  - `cache.py`: JSONL persistence + in-memory cache (`ResultsCache`).
  - `overlay.py`: shared overlay drawing (`OverlayRenderer`).
  - `exporter.py`: MP4 exporter reusing `OverlayRenderer` (`VideoExporter`).
  - `models.py`: data contracts (`Ball`, `FrameDetections`).

- `src/mill_presenter/ui/`
  - `main_window.py`: UI shell and controls wiring (`MainWindow`).
  - `widgets.py`: OpenGL-backed video surface (`VideoWidget`).
  - `playback_controller.py`: playback state machine (`PlaybackController`).
  - `calibration_controller.py`: 2-click calibration tool (`CalibrationController`).
  - `drum_calibration_controller.py`: drum auto-calibration UI (`DrumCalibrationController`).
  - `roi_controller.py`: ROI circle tool + ROI mask generator (`ROIController`).

### Config

- `configs/sample.config.yaml`
  - Contains size bins, calibration, overlay styles, vision parameters, and paths.

### CLI Scripts

- `scripts/run_detection.py`
  - Offline detection entry point.
  - Wires: `FrameLoader` + `VisionProcessor` + `ResultsCache` + `ProcessorOrchestrator`.

### Tests

- `tests/`
  - Unit tests for each major module and integration tests for the CLI and UI wiring.

---

## Data Contracts (Canonical Interfaces)

### `Ball` (One detected bead)

Defined in `src/mill_presenter/core/models.py`:

- `x: int` — center x in pixels (image coordinates).
- `y: int` — center y in pixels.
- `r_px: float` — detected radius in pixels.
- `diameter_mm: float` — computed diameter in mm from `px_per_mm`.
- `cls: int` — size label (4/6/8/10).
- `conf: float` — confidence score (heuristic).

### `FrameDetections` (All detections for a frame)

- `frame_id: int` — frame index aligned with decode order.
- `timestamp: float` — computed as `frame_id / fps`.
- `balls: list[Ball]`.

### On-disk cache format: JSONL

- One JSON object per line.
- Each line corresponds to exactly one frame.
- Written by `ResultsCache.save_frame()`.

---

## Video Ingestion (Decoding and Frame Indexing)

### `FrameLoader` (`core/playback.py`)

Responsibilities:

- Decode with PyAV (`av.open`).
- Apply rotation based on metadata (`rotate` tag or `DISPLAYMATRIX`).
- Provide frame iteration via `iter_frames(start_frame=0)`.
- Provide seeking via `seek(frame_index)`.

Key implementation notes:

- `iter_frames()` computes `current_idx` using PTS when available:
  - `current_idx = round((frame.pts * time_base) * fps)`
- After `seek()`, the decoder may yield pre-roll frames; `iter_frames()` skips frames until `current_idx >= start_frame`.

Comparison-relevant points:

- Rotation is applied immediately in `FrameLoader` so downstream modules operate in upright image coordinates.
- Frame index mapping is derived from timestamps, not a simple sequential counter.

---

## Offline Detection Pipeline

### Orchestration (`ProcessorOrchestrator` in `core/orchestrator.py`)

Flow:

1. Iterate frames from `FrameLoader.iter_frames()`.
2. Call `VisionProcessor.process_frame(frame_bgr, roi_mask=...)`.
3. Wrap the returned balls into `FrameDetections(frame_id, timestamp, balls)`.
4. Save via `ResultsCache.save_frame()`.

ROI mask injection:

- `ProcessorOrchestrator.set_roi_mask(mask)` stores a `numpy.ndarray` grayscale mask.
- The orchestrator passes that mask into the processor.

### Vision (`VisionProcessor` in `core/processor.py`)

#### Inputs

- `frame_bgr: np.ndarray` (HxWx3, BGR)
- `roi_mask: Optional[np.ndarray]` (grayscale uint8)

#### 1) Calibration and dynamic constraints

- Reads `calibration.px_per_mm` from config every frame.
- Uses a **dynamic pixel radius range** derived from physical assumptions:
  - `min_rad_mm = 2.5`, `max_rad_mm = 12.0`
  - `dyn_min_radius_px = int((min_rad_mm * px_per_mm) / 2)`
  - `dyn_max_radius_px = int((max_rad_mm * px_per_mm) / 2)`
- Sets center separation:
  - `dyn_min_dist_px = int(3.5 * px_per_mm)`

#### 2) Preprocessing

- Convert to grayscale.
- Bilateral filter: reduce glare/noise while preserving edges.
- CLAHE: local contrast boost.

#### 3) Detection proposals (dual path)

- Path A: `cv2.HoughCircles` on the enhanced image.
  - Uses `dyn_min_dist_px`, `dyn_min_radius_px`, `dyn_max_radius_px`.
  - Produces candidates with heuristic confidence 0.8.

- Path B: contour-based proposals.
  - Canny thresholds derived from Otsu.
  - Morphology close.
  - Find contours, filter by area and circularity (`vision.min_circularity`).
  - Fit `cv2.minEnclosingCircle()`.

#### 4) Filtering

Candidates are sorted by radius descending, then filtered in sequence:

1. **ROI filter**
   - If `roi_mask[y, x] == 0`, reject.
   - Only the candidate center point is checked (not full circle coverage).

2. **Brightness filter**
   - Computes mean brightness in a 5x5 patch around the center.
   - Rejects if `< 50` (intended to reject dark holes/shadows).

3. **Annulus (ring) logic**
   - If a smaller circle is near-centered inside a larger accepted circle, reject it as a hole:
     - `dist < fr * 0.5` and `r < fr * 0.8`.

4. **NMS (duplicate suppression)**
   - Rejects near-identical overlaps:
     - `dist < fr * 0.5` and `abs(r - fr) < fr * 0.3`.

#### 5) Classification

- Converts pixel radius to diameter in mm:
  - `diameter_mm = (2 * r_px) / px_per_mm`
- Assigns class using bins from config `bins_mm`.

If diameter does not fall in any bin, the current implementation prints a debug line.

---

## Result Persistence and Lookup

### `ResultsCache` (`core/cache.py`)

Responsibilities:

- Append-only JSONL writing during detection.
- Load JSONL into an in-memory dict: `frame_id -> FrameDetections`.
- Provide O(1) `get_frame(frame_id)` for playback/export.

Implementation notes:

- The current cache loads all frames into memory at startup if the file exists.
- Directory for the cache file is created automatically.

Comparison-relevant point:

- This design trades memory for deterministic, stutter-free UI lookup.

---

## Live Playback Pipeline (UI)

### UI entry point: `app.py`

- Loads YAML config.
- Creates `FrameLoader(video_path)`.
- Creates `ResultsCache(detections_path)` (loads JSONL into memory).
- Creates `MainWindow` and shows it.

### `PlaybackController` (`ui/playback_controller.py`)

Responsibilities:

- Drives playback using a `QTimer`.
- Each tick:
  1. Decode next frame using `FrameLoader.iter_frames(start_frame=...)`.
  2. Convert `np.ndarray` BGR to a **copied** `QImage`.
  3. Fetch detections via `ResultsCache.get_frame(frame_index)`.
  4. Send `(image, detections)` to `VideoWidget.set_frame()`.

Seeking:

- `seek(frame_index)` resets the iterator and uses a temporary iterator to fetch the requested frame.

### `VideoWidget` (`ui/widgets.py`)

Responsibilities:

- Render video frame + overlay + ROI/drum overlays.
- Maintain pan/zoom transform.
- Convert widget coordinates → image coordinates for tools.
- Emit interaction signals:
  - `mouse_pressed(x, y, is_left)`
  - `mouse_moved(x, y)`
  - `mouse_released(x, y)`

Overlay draw order in `paintEvent()`:

1. Draw video frame.
2. Draw detection circles via `OverlayRenderer`.
3. Draw ROI mask overlay (if active).
4. Draw drum calibration overlay (if active).
5. Draw calibration UI markers (if in calibration mode).

---

## Overlay Rendering

### `OverlayRenderer` (`core/overlay.py`)

Responsibilities:

- Pre-allocates `QPen` per class from config `overlay.colors`.
- Draws circles for balls whose `cls` is in `visible_classes`.

Current sizing rule:

- The UI draws using the **detected** radius:
  - `r_draw = ball.r_px * scale`

Comparison-relevant point:

- Overlay radius here is “what was detected” (pixel geometry), not a back-calculation from the class’s nominal mm size.

---

## ROI Mask System (Definition, Editing, and Usage)

### ROI editing (`ROIController`)

The ROI tool is implemented as an interactive **circle tool** (not freehand painting):

- On start:
  - Creates an ARGB mask `QImage` the same size as the current video frame.
  - Fills it with semi-transparent red.
  - Attempts to auto-detect the drum circle using HoughCircles.

- During editing:
  - Drag center region: move circle.
  - Drag rim region: resize circle.
  - Right click: reset.

- Rendering:
  - UI mask uses composition “clear” to make the circle region transparent.
  - A dashed yellow outline shows the boundary.

### ROI persistence (`roi_mask.png`)

On ROI mode exit, `MainWindow` saves the mask to:

- `paths.detections_dir/roi_mask.png` (default `./exports/roi_mask.png`).

Saved mask format:

- Grayscale image.
- White (255): valid.
- Black (0): ignored.

### ROI usage in detection (`VisionProcessor`)

During filtering, a candidate is rejected if its center is on a black pixel:

- `if roi_mask[y, x] == 0: continue`

### ROI usage in exporter (`VideoExporter`)

Exporter loads `roi_mask.png` and filters balls before drawing.

Note: `VideoExporter` currently uses a threshold `roi_mask[by, bx] > 200` to treat “valid”.

---

## Calibration

### Manual 2-point calibration (`CalibrationController`)

Workflow:

1. User toggles “Manual” mode.
2. User clicks two points.
3. User provides known distance in mm.
4. `px_per_mm` is computed and written into config:
   - `config['calibration']['px_per_mm'] = ratio`

### Drum calibration (`DrumCalibrationController`)

Workflow:

1. Auto-detect drum circle via Hough.
2. Refine circle by sampling radial edge gradients.
3. Show overlay for user confirmation/adjustment.
4. Confirm to compute and store `px_per_mm` based on known drum diameter.

---

## Export Pipeline

### `VideoExporter` (`core/exporter.py`)

Flow per frame:

1. Decode frame via `FrameLoader.iter_frames()`.
2. Load `FrameDetections` from `ResultsCache`.
3. Optionally filter balls by ROI mask.
4. Wrap the numpy BGR buffer in a `QImage` and draw with `OverlayRenderer`.
5. Write the modified frame via `cv2.VideoWriter`.

Comparison-relevant point:

- UI and exporter share the same `OverlayRenderer` to keep visuals consistent.

---

## Configuration Surface (Knobs You Compare Across Codebases)

### Bead size bins (`bins_mm`)

In `configs/sample.config.yaml`:

- 4 mm: 3.0–5.0
- 6 mm: 5.0–7.0
- 8 mm: 7.0–9.0
- 10 mm: 9.0–12.0

These bins are used by `VisionProcessor._classify_diameter()`.

### Calibration (`calibration.px_per_mm`)

Used for:

- Converting pixel radius to mm diameter.
- Constraining detection radii via `dyn_min_radius_px` / `dyn_max_radius_px`.

### Vision parameters (`vision.*`)

- `hough_param1`, `hough_param2`
- `min_dist_px` (currently overridden by dynamic min dist calculation)
- `min_circularity`

### Overlay styling (`overlay.*`)

- `overlay.colors` maps class → pen color.
- `overlay.line_width`.

---

## Performance Model (What Must Stay Fast)

- Offline detection is allowed to be slow (one-time cost).
- Live playback must be smooth:
  - No disk reads per frame (detections are loaded in memory).
  - No OpenCV detection during playback.
  - Overlay drawing is a simple loop over `detections.balls` with pre-allocated pens.

---

## Known Tradeoffs / Comparison Notes

1. **Center-point ROI filtering**
   - Mask is checked only at `(x, y)` of the candidate.
   - This is simple and fast but can allow circles that slightly cross into ignored areas.

2. **Overlay radius is pixel-based**
   - Current overlay draws `ball.r_px` directly.
   - If another codebase draws a class-nominal radius (mm → px), overlays will look different even for identical detections.

3. **Dynamic radius constraints depend on `px_per_mm`**
   - Detection radius range is derived from assumed physical bead sizes and `px_per_mm`.
   - This couples calibration with which circles are even detectable.

4. **Exporter ROI comment mismatch**
   - Exporter comments mention “Ignore as Gray (127)”, but ROI save path currently produces black (0) outside and white (255) inside.
   - Functionally it uses a `>200` threshold, which still works for black/white masks.

---

## Comparison Checklist (Questions to Answer About Both Codebases)

Use this checklist to compare the other implementation:

1. **Frame indexing**
   - How is `frame_id` determined? Sequential counter or PTS-based mapping?
   - Is seeking stable and repeatable?

2. **Rotation handling**
   - Where is rotation applied? Decoder layer vs vision layer vs UI layer?

3. **Cache contract**
   - What is the persistent output format (JSONL, CSV, protobuf, etc.)?
   - Is it append-only and crash-tolerant?
   - Is there an in-memory index for playback?

4. **Detection architecture**
   - Are detections computed offline only?
   - Are overlays ever recomputed at playback time?

5. **ROI semantics**
   - Is ROI applied per candidate center, per candidate area, or as a pixel-level mask on the whole image?
   - ROI storage: black/white? alpha? conventions?

6. **Size classification**
   - Does size mapping use explicit bins?
   - Are bins hard-coded or config-driven?
   - How is `px_per_mm` computed and used?

7. **Overlay semantics**
   - Is overlay radius the detected radius, or derived from class nominal diameter?
   - Do UI and exporter share the same rendering path?

8. **Performance strategy**
   - What operations run per frame during playback?
   - Any per-frame allocations or disk I/O?

---

## Pointers to Existing Deep Dives

If you need more detail on specific subsystems:

- `docs/technical_primer.md` — algorithm explanations and contributor workflow.
- `docs/design_decisions.md` — rationale behind architecture decisions.
- `docs/pipeline.md` — additional pipeline framing (may contain historical notes).
- `docs/roi_mask_system.md` — detailed ROI interaction and painting behavior.
